import numpy as np
import itertools
from copy import deepcopy
from collections import OrderedDict


def random_weight():
    """
    :return: float, number generated by np.random.normal, excluding 0.0
    """
    #parameters of np.random.normal (suggested by Bruno to be small values close to 0.0)
    average = 0.0
    stddev = 0.15

    value = 0.0
    while (value == 0.0):  # not really beautiful, but works
        value = np.random.normal(average, stddev)
    return value


class NeuralNet(object):
    """
    neuron(l,i) = neurons[l][i] [by definition, if i=0, it is a BIAS neuron]
    weight(l,i,j) = connections[l][(i,j)]
    """

    def __init__(self, input_size, output_size, hidden_layers_sizes, neurons_type='sigmoid', alpha=0.0001,
                 regularization=False):
        """
        :param input_size: int, size of an input
        :param output_size: int, size of an output
        :param hidden_layers_sizes: list of int, size of each hidden layer
        :param neurons_type: 'bias', 'sigmoid' or 'ReLU'
        :param alpha: float, alpha value
        :param regularization: bool, True if want to use Regularization, False if not
        """
        self.neurons, self.connections = self._generate_NN(input_size, output_size, hidden_layers_sizes, neurons_type)
        self.alpha = alpha
        self.regularization = regularization
        self.input_size = input_size
        self.hidden_layers_sizes = hidden_layers_sizes
        self.neuron_type = neurons_type

    def predict(self, nn_input: list):
        """
        :param nn_input: list of float (EXCLUDING CONSTANT FEATURE)
        :return: list of float, the predicted values
        """

        if len(nn_input) != self.input_size:
            raise ValueError("This neural network requires a list of %d elements, but %d were given" %
                             (self.input_size, len(nn_input)))

        activations = deepcopy(nn_input)
        activations = [1] + activations

        for layer_no, layer in enumerate(self.neurons):
            new_activations = []
            for neuron_no, neuron in enumerate(layer):
                activation_input = [a * w for a, w in zip(activations, self.connections[layer_no][neuron_no])]
                new_activations.append(neuron.activation(activation_input))
            activations = new_activations
            print(activations)

        return activations

    def back_propagation(self, error):  # TODO
        """
        :param error: the error of the NN, calculated outside
        remember to use self.regularization to see if need to use regularization or not
        probably will need to create smaller auxiliary function
        """
        pass

    @staticmethod
    def _generate_NN(input_size, output_size, hidden_layers_sizes, neurons_type='sigmoid'):
        # TODO transform this in a private method of NeuralNet
        """
        :param input_size: int, size of an input
        :param output_size: int, size of an output
        :param hidden_layers_sizes: list of int, size of each hidden layer
        :param neurons_type: 'bias', 'sigmoid' or 'ReLU'
        :return: neurons dict and connections dict
        """
        neurons = []
        connections =[]
        input_layer = [Neuron('bias')]
        input_layer += [Neuron(neurons_type) for _ in range(1, input_size + 1)]
        neurons.append(input_layer)

        for hidden_layer_size in hidden_layers_sizes:
            hidden_layer = [Neuron('bias')]
            hidden_layer += [Neuron(neurons_type) for _ in range(0, hidden_layer_size)]
            neurons.append(hidden_layer)

        neurons.append([Neuron(neurons_type) for _ in range(0, output_size)])

        #Iterates all layers except first because it is the input layer.
        for layer_no, layer in enumerate(neurons[1:], start=1):
            neurons_connections = []
            for _ in layer:
                a_neuron_connection = [random_weight() for _ in neurons[layer_no-1]]
                neurons_connections.append(a_neuron_connection)
            connections.append(neurons_connections)

        return neurons[1:], connections


class Neuron(object):
    def __init__(self, type):
        """
        :param type: 'bias' (always return 1.0), 'sigmoid' (1/1+exp(-x)) or 'ReLU' (max(0,x))
        (no need to calc delta for bias neurons)
        """
        self.type = type.upper()

    def activation(self, input_values=()):
        # if BIAS neuron, won't have input_values (input_values = weights * activations)
        """
        :param input_values: list of float, each value correspond to weight(i) * activation(i) (already calculated by the caller)
        :return: float, the activation output value
        """
        if self.type == 'BIAS':
            return 1.0
        else:
            if input_values == [] or input_values == ():
                raise ValueError('Input for sigmoid and ReLu can\'t be empty')

            x = sum(input_values)
            if self.type == 'SIGMOID':
                return 1.0 / (1.0 + np.exp(-x))
            elif self.type == 'RELU':
                return max(0.0, x)

    @staticmethod
    def delta(weights: list, deltas: list, activation: float):
        """
        WILL need input values (the weights, deltas and activation) (slide 56 aula 11)
        :return: the delta value (to calc the gradients to att the weights)
        """
        return sum([w * d for w, d in zip(weights, deltas)]) * activation * (1 - activation)
